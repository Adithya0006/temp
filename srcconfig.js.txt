
from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session
from sqlalchemy import func, case, distinct, cast, Date
from datetime import datetime
import statistics
@app.get("/dashboard/analytics")
def get_dashboard_analytics(db: Session = Depends(get_db)):
    """
    MASTER DASHBOARD ANALYTICS
    Returns:
    1. PCB Breakdown (Type & Status)
    2. Production Flow (WIP & Staff Capacity per Step)
    3. Operator Load (Active PCBs per Operator)
    4. Cycle Time (Average duration per Step)
    5. Batch Aging (Stuck/Old Batches)
    6. Daily Order Trend (Velocity) - NEW!
    """

    # Helper for date parsing
    def parse_db_date(date_str):
        if not date_str: return None
        try: return datetime.fromisoformat(str(date_str))
        except ValueError: return None

    # ======================================================
    # 1. PCB TYPE COUNTS
    # ======================================================
    type_counts = (
        db.query(PcbData.Type, func.count(PcbData.ID).label("count"))
        .group_by(PcbData.Type)
        .all()
    )
    pcb_type_distribution = {(row.Type or "Unknown"): row.count for row in type_counts}

    # ======================================================
    # 2. PCB STATUS COUNTS
    # ======================================================
    status_counts = (
        db.query(PcbData.status, func.count(PcbData.ID).label("count"))
        .group_by(PcbData.status)
        .all()
    )
    pcb_status_distribution = {(row.status or "Unknown"): row.count for row in status_counts}

    # ======================================================
    # 3. PRODUCTION FLOW METRICS
    # ======================================================
    flow_stats = (
        db.query(
            ProcessFlowMaster.flow_step_id,
            ProcessFlowMaster.step_name,
            ProcessFlowMaster.step_order,
            func.count(PCBAssignment.assignment_id).label("active_wip_count"),
            func.count(distinct(OperatorStepMapping.operator_staff_no)).label("qualified_staff_count")
        )
        .outerjoin(PCBAssignment, (PCBAssignment.current_step_id == ProcessFlowMaster.flow_step_id) & 
                                  (PCBAssignment.overall_status == "IN_PROGRESS"))
        .outerjoin(OperatorStepMapping, OperatorStepMapping.flow_step_id == ProcessFlowMaster.flow_step_id)
        .group_by(ProcessFlowMaster.flow_step_id, ProcessFlowMaster.step_name, ProcessFlowMaster.step_order)
        .order_by(ProcessFlowMaster.step_order)
        .all()
    )

    production_flow = []
    for row in flow_stats:
        production_flow.append({
            "step": row.step_name,
            "order": row.step_order,
            "active_pcbs": row.active_wip_count,
            "staff_capacity": row.qualified_staff_count
        })

    # ======================================================
    # 4. OPERATOR LOAD
    # ======================================================
    operator_counts = (
        db.query(
            PCBProcessLog.assigned_operator_staff_no,
            func.count(PCBAssignment.assignment_id).label("pcb_count")
        )
        .join(PCBAssignment, PCBAssignment.assignment_id == PCBProcessLog.assignment_id)
        .filter(
            PCBAssignment.overall_status == "IN_PROGRESS",
            PCBAssignment.current_step_id == PCBProcessLog.flow_step_id,
            PCBProcessLog.assigned_operator_staff_no != None
        )
        .group_by(PCBProcessLog.assigned_operator_staff_no)
        .all()
    )

    op_names = db.query(OperatorStepMapping.operator_staff_no, OperatorStepMapping.operator_name).distinct().all()
    name_lookup = {row.operator_staff_no: row.operator_name for row in op_names}

    operator_load_data = []
    for row in operator_counts:
        staff_no = row.assigned_operator_staff_no
        operator_load_data.append({
            "staff_no": staff_no,
            "name": name_lookup.get(staff_no, f"Staff {staff_no}"),
            "pcb_count": row.pcb_count
        })

    # ======================================================
    # 5. CYCLE TIME ANALYTICS
    # ======================================================
    time_logs = (
        db.query(ProcessFlowMaster.step_name, PCBProcessLog.start_time, PCBProcessLog.end_time)
        .join(ProcessFlowMaster, ProcessFlowMaster.flow_step_id == PCBProcessLog.flow_step_id)
        .filter(
            PCBProcessLog.process_status == "COMPLETED",
            PCBProcessLog.start_time.isnot(None),
            PCBProcessLog.end_time.isnot(None)
        )
        .all()
    )

    step_durations = {}
    for row in time_logs:
        start = parse_db_date(row.start_time)
        end = parse_db_date(row.end_time)
        if start and end and end > start:
            duration = (end - start).total_seconds() / 60.0
            step_durations.setdefault(row.step_name, []).append(duration)

    cycle_time_data = []
    for step, durations in step_durations.items():
        if durations:
            cycle_time_data.append({
                "step_name": step,
                "avg_time_minutes": round(statistics.mean(durations), 2),
                "sample_size": len(durations)
            })

    # ======================================================
    # 6. BATCH AGING (Stuck PCBs)
    # ======================================================
    active_assignments = (
        db.query(PCBAssignment.assignment_id, PCBAssignment.assigned_pcb_id, 
                 PCBAssignment.assignment_date, PCBAssignment.current_step_id)
        .filter(PCBAssignment.overall_status == "IN_PROGRESS")
        .all()
    )

    now = datetime.now()
    stuck_batches = []
    total_age = 0
    
    for row in active_assignments:
        assigned_date = parse_db_date(row.assignment_date)
        if assigned_date:
            age = (now - assigned_date).total_seconds() / 86400
            total_age += age
            if age > 5: # Threshold: 5 days
                stuck_batches.append({
                    "assignment_id": row.assignment_id,
                    "pcb_id": row.assigned_pcb_id,
                    "age_days": round(age, 1),
                    "current_step_id": row.current_step_id
                })
    
    avg_age = (total_age / len(active_assignments)) if active_assignments else 0

    # ======================================================
    # 7. DAILY ORDER TREND (Velocity) - NEW IMPLEMENTATION
    # Source: PcbData (createdAt)
    # ======================================================
    # Groups orders by Date(createdAt) to see daily input volume
    
    daily_trend_query = (
        db.query(
            cast(PcbData.createdAt, Date).label("date"),
            func.count(PcbData.ID).label("count")
        )
        .filter(PcbData.createdAt.isnot(None))
        .group_by(cast(PcbData.createdAt, Date))
        .order_by(cast(PcbData.createdAt, Date))
        .all()
    )

    order_trend_data = [
        {"date": str(row.date), "count": row.count} 
        for row in daily_trend_query
    ]

    # ======================================================
    # FINAL RETURN
    # ======================================================
    return {
        "pcb_types": pcb_type_distribution,
        "pcb_statuses": pcb_status_distribution,
        "production_flow": production_flow,
        "operator_load": operator_load_data,
        "cycle_time": cycle_time_data,
        "batch_aging": {
            "average_wip_age_days": round(avg_age, 1),
            "stuck_count": len(stuck_batches),
            "stuck_list": stuck_batches
        },
        "order_trend": order_trend_data  # <--- NEW DATA
    }
